{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w0P1rB6zdH5",
        "outputId": "0e740d7e-fc66-4ca6-cdc4-1680eb1b3bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBNU9C-T4p7t",
        "outputId": "1a90a5f9-ab26-444d-d258-70f67cd9e2be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "VIDEO_ROOT = '/content/drive/MyDrive/DAD/inner_mirror/'\n",
        "ANNOTATION_CSV = '/content/drive/MyDrive/DAD/activities/inner_mirror/objectlevel.chunks_90.csv'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/DAD/processed_frames/'"
      ],
      "metadata": {
        "id": "ovv3XMPHzoFh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load annotations\n",
        "df = pd.read_csv(ANNOTATION_CSV)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "EvWCbuhTz38C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over annotations\n",
        "\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    participant = row['participant_id']\n",
        "    file_id = row['file_id']\n",
        "    activity = row['activity']\n",
        "    start = int(row['frame_start'])\n",
        "    end = int(row['frame_end'])\n",
        "    chunk_id = row['chunk_id']\n",
        "\n",
        "\n",
        "    # Skip bad labels or unreadable files\n",
        "    if pd.isna(file_id) or activity == 'none':\n",
        "        continue\n",
        "\n",
        "    video_path = os.path.join(VIDEO_ROOT, file_id + '.mp4')\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Missing: {video_path}\")\n",
        "        continue\n",
        "\n",
        "    save_dir = os.path.join(OUTPUT_DIR, activity, f\"vp{participant}_{chunk_id}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "    frame_idx = 0\n",
        "\n",
        "    for fnum in range(start, end + 1):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        out_path = os.path.join(save_dir, f\"frame_{frame_idx:03d}.jpg\")\n",
        "        cv2.imwrite(out_path, frame)\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "49Izkoghz5fG",
        "outputId": "339713e7-6ca4-41e1-ec16-c570d62cdf43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/9969 [00:08<23:25:56,  8.46s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2803222465>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"frame_{frame_idx:03d}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mframe_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "CSV_PATH = \"/content/drive/MyDrive/DAD/clip_index.csv\"\n",
        "SOURCE_ROOT = \"/content/drive/MyDrive/DAD/processed_frames\"\n",
        "DEST_ROOT = \"/content/DAD_tensor_clips1\"\n",
        "FRAMES_PER_CLIP = 32\n",
        "IMAGE_SIZE = 112\n",
        "\n",
        "#Load clip list\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "to_tensor = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#Create target root\n",
        "os.makedirs(DEST_ROOT, exist_ok=True)\n",
        "\n",
        "failed = []\n",
        "\n",
        "#Convert each clip\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    label = row['label']\n",
        "    orig_path = row['clip_path']\n",
        "    clip_name = os.path.basename(orig_path)\n",
        "    source_folder = os.path.join(SOURCE_ROOT, label, clip_name)\n",
        "    dest_path = os.path.join(DEST_ROOT, label, clip_name + \".pt\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        frame_files = sorted([\n",
        "            f for f in os.listdir(source_folder)\n",
        "            if f.endswith(\".jpg\")\n",
        "        ])[:FRAMES_PER_CLIP]\n",
        "\n",
        "        frames = []\n",
        "        for fname in frame_files:\n",
        "            img = Image.open(os.path.join(source_folder, fname)).convert(\"RGB\")\n",
        "            img = to_tensor(img)\n",
        "            frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            raise ValueError(\"No valid frames\")\n",
        "\n",
        "        # Pad if fewer frames\n",
        "        while len(frames) < FRAMES_PER_CLIP:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "        clip_tensor = torch.stack(frames)  # [T, C, H, W]\n",
        "        torch.save(clip_tensor, dest_path)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed: {source_folder} â†’ {e}\")\n",
        "        failed.append(source_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtygDYJG4HnB",
        "outputId": "792f0fc8-e306-4dc4-cd59-5b50f446793d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 695/695 [1:04:50<00:00,  5.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/DAD/processed_frames/\"\n",
        "data = []\n",
        "\n",
        "for activity in os.listdir(ROOT):\n",
        "    act_dir = os.path.join(ROOT, activity)\n",
        "    if not os.path.isdir(act_dir):\n",
        "        continue\n",
        "    for clip in os.listdir(act_dir):\n",
        "        clip_path = os.path.join(act_dir, clip)\n",
        "        if os.path.isdir(clip_path):\n",
        "            data.append([clip_path, activity])\n",
        "\n",
        "df_clips = pd.DataFrame(data, columns=[\"clip_path\", \"label\"])\n",
        "df_clips.to_csv(\"/content/drive/MyDrive/DAD/clip_index.csv\", index=False)\n",
        "print(\"Saved: clip_index.csv\")\n",
        "\n",
        "# Replace frame-folder paths with .pt paths\n",
        "df_clips['clip_path'] = df_clips.apply(\n",
        "    lambda row: os.path.join(DEST_ROOT, row['label'], os.path.basename(row['clip_path']) + \".pt\"),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Optional: Save updated CSV\n",
        "df_clips.to_csv(\"/content/clip_index_pt.csv\", index=False)\n",
        "\n",
        "print(df_clips)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roGC2IYrz8WX",
        "outputId": "68cc410a-c757-4c4e-9f35-eff9e07cb430"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved: clip_index.csv\n",
            "                                            clip_path         label\n",
            "0    /content/DAD_tensor_clips1/reaching_for/vp1_0.pt  reaching_for\n",
            "1    /content/DAD_tensor_clips1/reaching_for/vp1_1.pt  reaching_for\n",
            "2    /content/DAD_tensor_clips1/reaching_for/vp2_0.pt  reaching_for\n",
            "3    /content/DAD_tensor_clips1/reaching_for/vp2_1.pt  reaching_for\n",
            "4    /content/DAD_tensor_clips1/reaching_for/vp3_0.pt  reaching_for\n",
            "..                                                ...           ...\n",
            "690      /content/DAD_tensor_clips1/closing/vp12_3.pt       closing\n",
            "691      /content/DAD_tensor_clips1/closing/vp12_4.pt       closing\n",
            "692      /content/DAD_tensor_clips1/closing/vp14_0.pt       closing\n",
            "693      /content/DAD_tensor_clips1/closing/vp14_1.pt       closing\n",
            "694      /content/DAD_tensor_clips1/closing/vp14_2.pt       closing\n",
            "\n",
            "[695 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define once globally\n",
        "CLASS_NAMES = ['closing', 'interacting', 'opening', 'placing_moving_to', 'reaching_for', 'retracting_from']\n",
        "label2id = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "df_clips['label_idx'] = df_clips['label'].map(label2id)"
      ],
      "metadata": {
        "id": "E_e3vCY3Qayj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ClipTensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        label = label2id[row['label']]  # now fixed globally\n",
        "        tensor = torch.load(row['clip_path'])  # [T, C, H, W]\n",
        "        return tensor.permute(1, 0, 2, 3), label\n",
        "\n"
      ],
      "metadata": {
        "id": "LL17Rteb3hg-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Stratify on integer label\n",
        "df_train, df_val = train_test_split(df_clips, test_size=0.2, stratify=df_clips['label_idx'], random_state=42)\n",
        "\n",
        "#Dataset definition should use 'label_idx', not 'label'\n",
        "train_ds = ClipTensorDataset(df_train)\n",
        "val_ds = ClipTensorDataset(df_val)\n",
        "\n",
        "#Normalization for visual models\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "#Weighted sampler on label_idx (int class labels)\n",
        "class_counts = df_train['label_idx'].value_counts()\n",
        "class_weights = 1. / class_counts\n",
        "weights = df_train['label_idx'].map(class_weights).values\n",
        "\n",
        "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "\n",
        "#Dataloaders\n",
        "train_loader = DataLoader(train_ds, batch_size=4, sampler=sampler, num_workers=4)\n",
        "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "trOS1wdO3kD5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models.video as video_models\n",
        "\n",
        "NUM_CLASSES = df['label'].nunique()\n",
        "\n",
        "model = video_models.r3d_18(pretrained=False)  # or pretrained=True for Kinetics\n",
        "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvF5l4jV3p6S",
        "outputId": "7ed48ace-4da3-45a6-802b-1b2d8c40350d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "Y5oFvfQK4P4R"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for clips, labels in loader:\n",
        "            clips, labels = clips.to(device), labels.to(device)\n",
        "            outputs = model(clips)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train(model, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total, correct = 0, 0\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "        for clips, labels in loop:\n",
        "            clips, labels = clips.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            loop.set_postfix(train_acc=correct/total)\n",
        "\n",
        "        val_acc = evaluate(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1} | Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "8oqE7JzG4vel"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading one batch for debug...\")\n",
        "for clips, labels in train_loader:\n",
        "    print(f\"[DEBUG] Loaded batch of shape: {clips.shape}\")\n",
        "    print(f\"[DEBUG] Labels: {labels}\")\n",
        "    break  # just test the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Kxczxy3hZd",
        "outputId": "55b92269-8bc2-44ac-bbb4-f1342e00e39e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading one batch for debug...\n",
            "[DEBUG] Loaded batch of shape: torch.Size([4, 3, 32, 112, 112])\n",
            "[DEBUG] Labels: tensor([5, 5, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeU_AtuV4xrt",
        "outputId": "701e9de0-6c8d-426b-97af-2907974a5ec0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.09it/s, train_acc=0.358]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Val Acc: 0.5468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.02it/s, train_acc=0.556]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Val Acc: 0.5612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.22it/s, train_acc=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Val Acc: 0.2230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.18it/s, train_acc=0.82]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Val Acc: 0.6691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.10it/s, train_acc=0.867]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Val Acc: 0.7266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.23it/s, train_acc=0.921]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Val Acc: 0.6906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.23it/s, train_acc=0.892]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Val Acc: 0.6835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:09<00:00, 14.27it/s, train_acc=0.962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Val Acc: 0.7626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os, cv2, torch\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "\n",
        "# === CONFIG ===\n",
        "CSV_PATH = \"/content/drive/MyDrive/DAD/clip_index.csv\"\n",
        "FRAMES_DIR = \"/content/drive/MyDrive/DAD/processed_frames\"\n",
        "OUT_VIDEO = \"/content/annotated_from_frames1.mp4\"\n",
        "MAX_FRAMES = 2000\n",
        "FPS = 5\n",
        "FRAME_SIZE = (112, 112)\n",
        "\n",
        "# === Label Mapping ===\n",
        "CLASS_NAMES = ['closing', 'interacting', 'opening', 'placing_moving_to', 'reaching_for', 'retracting_from']\n",
        "label2id = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "# === Model should already be loaded ===\n",
        "model.eval()\n",
        "\n",
        "# === Transform for inference ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(FRAME_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# === Prediction helper ===\n",
        "def predict_clip(frames):\n",
        "    frames_tensor = [transform(f) for f in frames]\n",
        "    clip = torch.stack(frames_tensor).permute(1, 0, 2, 3).unsqueeze(0).to(device)  # [1, C, T, H, W]\n",
        "    with torch.no_grad():\n",
        "        output = model(clip)\n",
        "        probs = torch.softmax(output, dim=1).squeeze().cpu().numpy()\n",
        "        top_idx = probs.argmax()\n",
        "        top_label = id2label[top_idx]\n",
        "        print(dict(zip(CLASS_NAMES, probs.round(3))))  # Optional debug\n",
        "        return top_label, probs[top_idx]\n",
        "\n",
        "# === Annotate video ===\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Make sure ground truth labels are interpreted correctly\n",
        "if 'label_idx' not in df.columns:\n",
        "    df['label_idx'] = df['label'].map(label2id)\n",
        "\n",
        "df_sample = df.sample(5, random_state=246).reset_index(drop=True)\n",
        "\n",
        "video_writer = None\n",
        "frame_count = 0\n",
        "\n",
        "for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
        "    if frame_count >= MAX_FRAMES:\n",
        "        break\n",
        "\n",
        "    clip_path = row['clip_path']\n",
        "    gt_label_idx = row['label_idx']\n",
        "    gt_label = id2label[gt_label_idx]\n",
        "\n",
        "    full_path = os.path.join(FRAMES_DIR, clip_path)\n",
        "    frame_files = sorted(os.listdir(full_path))\n",
        "    rgb_frames = []\n",
        "\n",
        "    for f in frame_files:\n",
        "        img = cv2.imread(os.path.join(full_path, f))\n",
        "        if img is None:\n",
        "            continue\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        rgb_frames.append(img_rgb)\n",
        "\n",
        "    if len(rgb_frames) == 0:\n",
        "        continue\n",
        "\n",
        "    pred_label, pred_conf = predict_clip(rgb_frames)\n",
        "\n",
        "    for img in rgb_frames[::2]:  # annotate every 2nd frame\n",
        "        if frame_count >= MAX_FRAMES:\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "        cv2.putText(frame, f\"GT: {gt_label}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
        "        cv2.putText(frame, f\"Pred: {pred_label} ({pred_conf:.2f})\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,100,255), 2)\n",
        "\n",
        "        if video_writer is None:\n",
        "            h, w = frame.shape[:2]\n",
        "            video_writer = cv2.VideoWriter(OUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), FPS, (w, h))\n",
        "\n",
        "        video_writer.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "if video_writer:\n",
        "    video_writer.release()\n",
        "\n",
        "print(f\"ðŸ“¼ Saved video (limited to {MAX_FRAMES} frames): {OUT_VIDEO}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHp4KsPr8-3n",
        "outputId": "609ce5d5-4db5-4cb0-b917-99f00a713dd3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:00<01:14, 24.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'closing': np.float32(0.003), 'interacting': np.float32(0.655), 'opening': np.float32(0.0), 'placing_moving_to': np.float32(0.004), 'reaching_for': np.float32(0.338), 'retracting_from': np.float32(0.0)}\n",
            "{'closing': np.float32(0.004), 'interacting': np.float32(0.808), 'opening': np.float32(0.0), 'placing_moving_to': np.float32(0.034), 'reaching_for': np.float32(0.153), 'retracting_from': np.float32(0.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:22<00:46, 23.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'closing': np.float32(0.004), 'interacting': np.float32(0.605), 'opening': np.float32(0.0), 'placing_moving_to': np.float32(0.018), 'reaching_for': np.float32(0.373), 'retracting_from': np.float32(0.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:11<00:33, 33.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'closing': np.float32(0.001), 'interacting': np.float32(0.833), 'opening': np.float32(0.0), 'placing_moving_to': np.float32(0.002), 'reaching_for': np.float32(0.164), 'retracting_from': np.float32(0.0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [02:56<00:00, 35.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'closing': np.float32(0.003), 'interacting': np.float32(0.859), 'opening': np.float32(0.0), 'placing_moving_to': np.float32(0.012), 'reaching_for': np.float32(0.126), 'retracting_from': np.float32(0.0)}\n",
            "ðŸ“¼ Saved video (limited to 2000 frames): /content/annotated_from_frames1.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eiNS9nXK8MkU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}